#!/usr/bin/env python3
from random import sample
import numpy as np
import signal
import matplotlib.pyplot as plt
from geometry_msgs.msg import Twist
import time
from pynput import keyboard
import PyKDL
import hmac
import hashlib
import base64
import secrets
import ctypes
import struct
from core_robotics.PyBSpline import convertToUV, BSplineSurface
from corrective_shared_autonomy.TaskModels.DMPLWRhardcoded import HybridSegment, DMPLWRhardcoded
from scipy.spatial.transform import Rotation as ScipyR
from corrective_shared_autonomy.TaskModels.FragmentedExecution import getFragmentedTraj, constructConnectedTraj, interpMultD
import cv2
from threading import Thread

from cv_bridge import CvBridge

import sys
import json

import rospy
from geometry_msgs.msg import Twist
import numpy as np
import rospkg
import ros_numpy
import sensor_msgs.point_cloud2 as pc2
from sensor_msgs.msg import Image, PointCloud2, CameraInfo, JointState, CompressedImage,PointCloud2
from geometry_msgs.msg import TransformStamped, PoseStamped, Point, Pose, PoseArray, Quaternion, Twist, Vector3, PointStamped
from visualization_msgs.msg import MarkerArray
from std_msgs.msg import String, Bool
from nav_msgs.msg import Path
from scipy.spatial.transform import Slerp

import image_geometry
from tf.transformations import quaternion_from_euler
import tf
import tf2_ros
import ctypes
import threading

import tf2_geometry_msgs
from scipy.spatial.transform import Rotation as ScipyR

from franka_core_msgs.msg import RobotState

from corrective_shared_autonomy.TaskModels.FragmentedExecution import checkReachabilityOfPoses


REFERENCE_FRAME='panda_link0'
CAMERA_FRAME='rgb_camera_link'

def get_mid_points(p1,p2,k):
    return [[p1[0]+i*(p2[0]-p1[0])/(k-1),p1[1]+i*(p2[1]-p1[1])/(k-1)] for i in range(k)]

def get_distance(p1,p2):
    return np.sqrt((p2[0]-p1[0])**2+(p2[1]-p1[1])**2+(p2[2]-p1[2])**2)

# https://www.geeksforgeeks.org/python-different-ways-to-kill-a-thread/
class thread_with_trace(threading.Thread):
  def __init__(self, *args, **keywords):
    threading.Thread.__init__(self, *args, **keywords)
    self.killed = False
 
  def start(self):
    self.__run_backup = self.run
    self.run = self.__run     
    threading.Thread.start(self)
 
  def __run(self):
    sys.settrace(self.globaltrace)
    self.__run_backup()
    self.run = self.__run_backup
 
  def globaltrace(self, frame, event, arg):
    if event == 'call':
      return self.localtrace
    else:
      return None
 
  def localtrace(self, frame, event, arg):
    if self.killed:
      if event == 'line':
        raise SystemExit()
    return self.localtrace
 
  def kill(self):
    self.killed = True

class Spliner(object):
    def __init__(self):
        self._tfBuffer = tf2_ros.Buffer()
        self._tl = tf2_ros.TransformListener(self._tfBuffer)
        self._camera_model = None
        self._bridge = CvBridge()
        self._depth_stale = True

        self._depth = None
        self._camera_model = None
        self._learnedSegments = None
        self._scanning = False
        self._downsamp = 40
        self.resume = False # resume execution after paused
        self._abort = False
        self._path_pub = rospy.Publisher("/path",Path,queue_size = 1)
        self._ui_path_pub = rospy.Publisher("/ui/path",String,queue_size = 1)
        self._ui_feedback_pub = rospy.Publisher("/ui/feedback",String,queue_size = 1)
        self._ui_reach_pub = rospy.Publisher("/ui/reach",String,queue_size = 1)
        self._ui_robot_state_pub = rospy.Publisher("/ui/robot_state",String,queue_size = 1)
        self._events_pub = rospy.Publisher("/interaction_events",String,queue_size = 1)
        self._points_pub = rospy.Publisher("/clicked_point",PointStamped,queue_size = 1)
        self._pose_pub = rospy.Publisher("/checked_poses", PoseArray,queue_size=1)

        # self._pose_pub2 = rospy.Publisher("/_poses", PoseArray,queue_size=1)


        self._reach_thread = None

        self._n_passes = 2
        self._horizontal = True
        self._time_sampling = 20.0
        self._pitch = 0

        self._surfaceBSpline = None
        self._biased = False
        self._robot_state = "grey"
        self._pattern = "None"

        self._depth_sub = None
        self._corners_sub = None

        self._bias_sub = rospy.Subscriber("/ft/bias", Bool, self.on_bias)
        self._robot_state_sub = rospy.Subscriber("/franka_ros_interface/custom_franka_state_controller/robot_state", RobotState, self.on_state)
        self._cam_info_sub = rospy.Subscriber("/k4a/depth_to_rgb/camera_info", CameraInfo, self.on_info)
        self._rviz_trigger_sub = rospy.Subscriber("/rviz_triggers", String, self.on_trigger)
        self._ui_param_sub = rospy.Subscriber("/ui/parameters", String, self.on_param)
        self.resumeexec = rospy.Subscriber("/execution/interrupt", String, self.checkResume)
        self._reach_map_sub = rospy.Subscriber("/reachabilitymap", MarkerArray, self.on_map)
        self._command_sub = rospy.Subscriber("/ui/commands",String, self.on_command)


    def on_bias(self, msg):
        self._biased = msg.data

    def on_map(self, msg):
        if len(msg.markers) == 0:
            return
        print("got map")
        good = []
        bad = []
        print(len(msg.markers))
        for marker in msg.markers:
            pose = PoseStamped()
            pose.header.frame_id=REFERENCE_FRAME
            pose.pose = marker.pose
            # print(pose)
            coord = self.get_pixel_from_pose(pose)
            if coord is None:
                continue
            coord=["{:.3f}".format(coord[0]/self._width),"{:.3f}".format(coord[1]/self._height)]
            if marker.color.r < .8:
                good.append(','.join(coord))
            else:
                bad.append(','.join(coord))
        
        self._ui_reach_pub.publish("good:"+';'.join(good))
        rospy.sleep(0.1)
        self._ui_reach_pub.publish("bad:"+';'.join(bad))

    def on_trigger(self, msg):
        if msg.data == "scan":
            self._ui_feedback_pub.publish("8;Loading camera driver")
            self._scanning = True
            self._depth_sub.unregister()
            self._corners_sub.unregister()
        if msg.data == "camera_driver_running":
            self._ui_feedback_pub.publish("10;Scanning")
        if msg.data == "scanningdone":
            self.register_depth()
            self._ui_feedback_pub.publish("10;Move target to piece location")
            
    def checkResume(self,data):
        if data.data=='resume':
            self.resume = True

    def on_state(self, msg):
        if (msg.robot_mode == 1 or msg.robot_mode ==2): # idle or moving (https://frankaemika.github.io/libfranka/robot__state_8h.html#adfe059ae23ebbad59e421edaa879651a)
            if self._biased:
                self._robot_state = "green"
            else: 
                self._robot_state = "purple"
        if msg.robot_mode == 4:
            self._robot_state = "red"
        if msg.robot_mode == 5:
            self._robot_state = "orange"
        self._ui_robot_state_pub.publish(self._robot_state)
        
    def get_param(self, label):
        idx = next((i for i, item in enumerate(self._param) if item["id"] == label), None)
        if idx is not None:
            return self._param[idx]["value"]
        else:
            return None

    def on_param(self,msg):
        self._param = json.loads(msg.data)
        print(self._param)
        
        passes = self.get_param("passes")
        if passes is not None:
            self._n_passes = int(passes)
        else:
            self._n_passes = 2

        force = self.get_param("force")
        if force is not None:
            self._force = -int(force)
        else:
            self._force = -5.

        self._pattern = self.get_param("pattern")
        print("----------------------------------------")
        print(self._pattern)

        pitch = self.get_param("pitch")
        if pitch is not None:
            self._pitch = int(pitch)
        else:
            self._pitch = 0

        velocity = self.get_param("speed")
        if velocity is not None:
            self._velocity = int(velocity)/1000.
        else:
            self._velocity = .01

        self._horizontal = self.get_param("direction") == "Horizontal"
        self._ui_robot_state_pub.publish(self._robot_state)

    def register_depth(self):
        self._depth_sub = rospy.Subscriber("/k4a/depth_to_rgb/image_raw/compressed", CompressedImage, self.on_depth)

    def on_info(self,msg):
        if self._camera_model is None:
            self._width = msg.width
            self._height = msg.height
            self._camera_model = image_geometry.PinholeCameraModel()
            self._camera_model.fromCameraInfo(msg)
            
        if self._depth_sub is None and not self._scanning:
            self.register_depth()
    
    def linearization(self,x):
        return 1/(1+np.exp(-2.2*(2*x-1)))/0.7964776354986698-0.12523953551719247
        
    def get_reach(self, p):
        print("checking reachability")
        reachable = checkReachabilityOfPoses(p)
        print("checked reachability")
        good = []
        bad = []
        for index,reach in enumerate(reachable):
            pose = PoseStamped()
            pose.header.frame_id=REFERENCE_FRAME
            pose.pose.position.x = p[0,index]
            pose.pose.position.y = p[1,index]
            pose.pose.position.z = p[2,index]
            pose.pose.orientation.x = p[3,index]
            pose.pose.orientation.y = p[4,index]
            pose.pose.orientation.z = p[5,index]
            pose.pose.orientation.w = p[6,index]
            # print(pose)
            coord = self.get_pixel_from_pose(pose)
            if coord is None:
                continue
            coord=[str(coord[0]/self._width),str(coord[1]/self._height)]
            if reach:
                good.append(','.join(coord))
            else:
                bad.append(','.join(coord))
        
        self._ui_reach_pub.publish("good:"+';'.join(good))
        rospy.sleep(0.1)
        self._ui_reach_pub.publish("bad:"+';'.join(bad))
        self._reach_thread = None
        self._ui_feedback_pub.publish("4;Reach computed")

    def on_command(self,msg):
        print(msg.data)
        l = msg.data.split(":")
        if l[0] == "abort":
            self._abort = True
        if l[0] == "resume":
            self.resume = True
        if l[0] == "pause":
            self.resume = False
        if l[0] == "stop_reach":
            if self._reach_thread is not None:
                print("killing thread")
                self._reach_thread.kill()
        if l[0] == "spline":
            if self._depth_sub is None:
                self._ui_feedback_pub("No depth received")
                return
            self._surfaceBSpline = self.get_surface(l[1],REFERENCE_FRAME)
            print("splining finished")
            k=15
            p=np.zeros((7,k*k))
            R_tool_surf = ScipyR.from_euler('z',180,degrees=True)
            try:
                for u in range(k):
                    for v in range(k):
                        x=min(.999,self.linearization(u/float(k-1)))
                        y=min(.999,self.linearization(v/float(k-1)))
                        pt, n_hat, r_u_norm, r_v_norm = self._surfaceBSpline.calculate_surface_point(x, y)
                        R_surf = ScipyR.from_matrix(np.hstack([r_u_norm.reshape((3,1)), r_v_norm.reshape((3,1)), n_hat.reshape((3,1))]))
                        q = (R_surf * R_tool_surf).as_quat()
                        p[:,u*k+v]=np.array([pt[0],pt[1],pt[2],q[0],q[1],q[2],q[3]])
            except:
                self._ui_feedback_pub.publish("4;Error, out of bounds")
                return
            self._ui_feedback_pub.publish("20;Computing reach")

            # print(p)
            ros_poses = PoseArray()
            ros_poses.header.stamp = rospy.Time.now()
            ros_poses.header.frame_id = "panda_link0"

            for pose in np.array(p).T:
                pp = Pose()
                pp.position.x = pose[0]
                pp.position.y = pose[1]
                pp.position.z = pose[2]
                pp.orientation.x = pose[3]
                pp.orientation.y = pose[4]
                pp.orientation.z = pose[5]
                pp.orientation.w = pose[6]
                ros_poses.poses.append(pp)
            
            self._pose_pub.publish(ros_poses)
            rospy.sleep(.01)
            if self._reach_thread is not None:
                self._reach_thread.kill()
            self._reach_thread = thread_with_trace(target = self.get_reach, args=(p,))
            self._reach_thread.start()

        if l[0] == "get_path":
            if self._surfaceBSpline is None:
                return
            self._ui_feedback_pub.publish("40;Computing path")
            path = self.get_path()
            ratio_points = []
            for p in path.poses:
                # print(p)
                coord = self.get_pixel_from_pose(p)
                coord=[str(coord[0]/self._width),str(coord[1]/self._height)]
                ratio_points.append(','.join(coord))
            
            self._ui_path_pub.publish(String(';'.join(ratio_points)))
            self._path_pub.publish(path)
            self._ui_feedback_pub.publish("2;Path computed")
            print("path finished")
        if l[0] == "push":
            print("push")
            x = int(float(l[1].split(",")[0])*self._width)
            y = int(float(l[1].split(",")[1])*self._height)
            p = self.get_point([x,y],CAMERA_FRAME)
        if l[0] == "publish_point":
            print("point")
            x = int(float(l[1].split(",")[0])*self._width)
            y = int(float(l[1].split(",")[1])*self._height)
            p = self.get_point([x,y])
            point = PointStamped()
            point.header.frame_id = REFERENCE_FRAME
            point.header.stamp = rospy.Time.now()
            point.point.x = p[0]
            point.point.y = p[1]
            point.point.z = p[2]
            self._points_pub.publish(point)
            # print(p)
        if l[0] == "execute":
            if self._learnedSegments is not None:
                # self._ui_feedback_pub.publish("2000;Executing")
                print("EXECUTING")
                model = DMPLWRhardcoded(verbose=True, dt=1.0/self._time_sampling)
                # model.executeModel(learnedSegments=self._learnedSegments, input_type='1dof')
                doneExecution = False
                segID=0; s=0
                while not doneExecution and not self._abort:
                    segID, s = model.executeModel(learnedSegments=self._learnedSegments, input_type='mdof', segID_start=segID, s_start=s)
                    if segID==-1 and s==-1:
                        doneExecution = True
                    else:
                        while not self.resume or not self._abort:
                            print("waiting")
                            time.sleep(0.5)
                        self.resume = False
                self._abort = False
                print("DONE EXECUTING")
                self._ui_feedback_pub.publish("4;Finished executing")
                # trans = self._tfBuffer.lookup_transform(REFERENCE_FRAME,REFERENCE_FRAME, rospy.Time())

                # q=trans.transform.rotation
                # t=trans.transform.translation
                # R_surface=[q.x,q.y,q.z,q.w]
                # t_surface=[t.x,t.y,t.z]

                # self._model.executeModel(learnedSegments=self._learnedSegments,R_surface=R_surface,t_surface=t_surface,input_type="1dof")

            
    def get_xyz_from_pixel(self, x,y,d):
        
        p = [x,y]
        p = self._camera_model.rectifyPoint(p)
        p3d = self._camera_model.projectPixelTo3dRay(p)
        point = Point()
        point.x=d/p3d[2]*p3d[0]
        point.y=d/p3d[2]*p3d[1]
        point.z=d/p3d[2]*p3d[2]
        return point

    def get_close_point(self, x,y):
        dist = 1
        while(True):
            for i in range(-dist,dist):
                if self._depth[x+i,y-dist] > 0:
                    return x+i,y-dist,self._depth[y-dist,x+i]
                if self._depth[x+i,y+dist] > 0:
                    return x+i,y+dist,self._depth[y+dist,x+i]
                if self._depth[x+dist,y+i] > 0:
                    return x+dist,y+i,self._depth[y+i,x+dist]
                if self._depth[x-dist,y+i] > 0:
                    return x-dist,y+i,self._depth[y+i,x-dist]
            dist+=1

    def get_pose_from_pixel(self, x,y,frame_id=REFERENCE_FRAME):
        if self._depth_stale is True:
            self._depth_stale = False
            self._depth = np.asarray(self._bridge.compressed_imgmsg_to_cv2(self._depth_msg))
        x = int(x)
        y=int(y)
        d = self._depth[y,x]/1000.
        if not d > 0:
            x,y,d = self.get_close_point(x,y)
            
        point = self.get_xyz_from_pixel(x,y,d)
        p = PoseStamped()
        p.header.frame_id = CAMERA_FRAME
        p.header.stamp = rospy.Time(0)
        p.pose.position = point
        p.pose.orientation.w = 1
        return self._tfBuffer.transform(p,frame_id)

    def get_point(self,p,frame_id = REFERENCE_FRAME):
        p = self.get_pose_from_pixel(p[0],p[1], frame_id)
        if p is None:
            return
        return [p.pose.position.x, p.pose.position.y,p.pose.position.z]

    def on_depth(self, msg):
        # print(rospy.Time.now(),": got depth")
        self._depth_msg = msg 
        self._depth_stale = True
    
    def get_surface(self,coord, frame_id = CAMERA_FRAME):
        #s="484,436_615,436_615,816_484,816"

        ps = coord.split(";")

        p1 = np.array([int(float(ps[0].split(',')[0])*self._width),int(float(ps[0].split(',')[1])*self._height)])
        p2 = np.array([int(float(ps[1].split(',')[0])*self._width),int(float(ps[1].split(',')[1])*self._height)])
        p3 = np.array([int(float(ps[2].split(',')[0])*self._width),int(float(ps[2].split(',')[1])*self._height)])
        p4 = np.array([int(float(ps[3].split(',')[0])*self._width),int(float(ps[3].split(',')[1])*self._height)])
        points = [p1,p2,p3,p4]
        dp = np.array([np.linalg.norm(p) for p in points])
        start_index = np.argmin(dp)
        if points[(start_index + 1)%4][1] > points[start_index-1][1]:
            pp1 = points[start_index]
            pp2 = points[(start_index + 1)%4]
            pp3 = points[(start_index + 2)%4]
            pp4 = points[(start_index + 3)%4]
        else:
            pp1 = points[start_index]
            pp2 = points[start_index - 1]
            pp3 = points[start_index - 2]
            pp4 = points[start_index - 3]
        #print("p1 "+str(p1))
        #print("p2 "+str(p2))
        #print("p3 "+str(p3))
        #print("p4 "+str(p4))
        #print("pp1 "+str(pp1))
        #print("pp2 "+str(pp2))
        #print("pp3 "+str(pp3))
        #print("pp4 "+str(pp4))
        pose1 = self.get_point(pp1,frame_id)
        pose2 = self.get_point(pp2,frame_id)
        pose3 = self.get_point(pp3,frame_id)
        pose4 = self.get_point(pp4,frame_id)
        
        dt = get_distance(pose1,pose2)
        dr = get_distance(pose2,pose3)
        db = get_distance(pose3,pose4)
        dl = get_distance(pose4,pose1)


        # spline_step = .03
        
        step_x = 10#int(max(dt,db)/spline_step)+1
        step_y = 10#int(max(dr,dl)/spline_step)+1

        top = get_mid_points(pp1,pp2,step_x)
        bot = get_mid_points(pp4,pp3,step_x)
        
        p=[]
        for i in range(step_x):
            p = p + get_mid_points(top[i],bot[i],step_y)
        p = [self.get_point(k,frame_id) for k in p]

        
        # ros_poses = PoseArray()
        # ros_poses.header.stamp = rospy.Time.now()
        # ros_poses.header.frame_id = CAMERA_FRAME
        # for pose in np.array(p):
        #     pp = Pose()
        #     pp.position.x = pose[0]
        #     pp.position.y = pose[1]
        #     pp.position.z = pose[2]
        #     pp.orientation.x = 0
        #     pp.orientation.y = 0
        #     pp.orientation.z = 0
        #     pp.orientation.w = 1
        #     ros_poses.poses.append(pp)
        
        # self._pose_pub2.publish(ros_poses)

        ctrl_p = np.array(p).reshape((step_x,step_y,3))

        surfaceBSpline = BSplineSurface()
        surfaceBSpline.initialize(k=3, control_pts=ctrl_p)

        return surfaceBSpline

    
    def get_segments(self, surfaceBSpline,us,vs):
        print("pitch", self._pitch)
        if self._horizontal:
            R_tool_surf = ScipyR.from_euler('zx',[180,self._pitch],degrees=True)
        else:
            R_tool_surf = ScipyR.from_euler('zy',[180,self._pitch],degrees=True)
        q_ts = R_tool_surf.as_quat()
        #samps = int(20 * distance / self._velocity)

        hybrid = True
        surface = surfaceBSpline
        state_names = ['u','v','f','theta_qx','theta_qy','theta_qz','theta_qw','delta_s','valve','tool_offset_x','tool_offset_y','tool_offset_z']
        original_vals = []
        num_samples = 0
        state_vals = []
        for i in range(int(len(us)/2)):
            start, _, _, _ = self._surfaceBSpline.calculate_surface_point(us[2*i], vs[2*i])
            end, _, _, _ = self._surfaceBSpline.calculate_surface_point(us[2*i+1], vs[2*i+1])
            distance = np.linalg.norm(start-end)
            samps = int(20 * distance / self._velocity)
            state_val = np.zeros((len(state_names),samps)) 

                

            starting = [us[2*i], vs[2*i],self._force, q_ts[0], q_ts[1], q_ts[2], q_ts[3], 1, 1, -self._pitch/150. if not self._horizontal else 0., self._pitch/150. if self._horizontal else 0., 0]
            ending = [us[2*i+1], vs[2*i+1],self._force, q_ts[0], q_ts[1], q_ts[2], q_ts[3], 1, 1, -self._pitch/150. if not self._horizontal else 0., self._pitch/150. if self._horizontal else 0., 0]

            if self._pattern is None or self._pattern == "None":
                orig_vals = interpMultD(starting,ending,samps,quat_vars=[3])
            else:
                print("Adding circles :O")
                orig_vals = interpMultD(starting,ending,samps,quat_vars=[3],super_pos_vars=[0], super_pos_freq=[0.04], super_pos_amp=[0.03])

            state_vals.append(orig_vals)

        temp_masks, trajs, _ = getFragmentedTraj(surfaceBSpline,state_names,state_vals,np.array([0,0,0,1]),np.zeros((3,)),3,downsamp = self._downsamp)
        # for t in trajs:
        #     for v in t:
        #         print(v)
        if self._horizontal:
            R_x = ScipyR.from_euler('y',-50,degrees=True)
            R_y = ScipyR.from_euler('x',10,degrees=True)
            tx1=0.02
            tx2=0
            ty1=0.
            ty2=-0.005
        else:
            R_x = ScipyR.from_euler('x',-10,degrees=True)
            R_y = ScipyR.from_euler('y',-50,degrees=True)
            tx1=0.
            tx2=0.
            ty1=-0.02
            ty2=0.
        
        q_x = R_x.as_quat()
        q_y = R_y.as_quat()

        corrections = []
        # Orthogonal
        if self._horizontal:
            corrections.append(np.array([1./(2*self._n_passes), 0.0, 0., 0, 0, 0, 1, 0, 0.0, 0,0,0]))
        else:
            corrections.append(np.array([0., -0.1/(2*self._n_passes), 0., 0, 0, 0, 1, 0, 0.0, 0,0,0]))
        # Abbrasiveness
        corrections.append(np.array([0.0, 0.0, -10.0, 0, 0, 0, 1, -0.5, 0.0, 0,0,0]))
        # # y
        # corrections.append(np.array([0., 0.1, 0., 0, 0, 0, 1, 0, 0.0]))
        # tx
        corrections.append(np.array([0.0, 0.0, 0.0, q_x[0], q_x[1], q_x[2], q_x[3], 0.0, 0.0, tx1,ty1,0]))
        # ty
        corrections.append(np.array([0.0, 0.0, 0.0, q_y[0], q_y[1], q_y[2], q_y[3], 0.0, 0.0, tx2,ty2,0]))
        corrections.append(np.array([0.0, 0.0, 0.0, 0, 0, 0, 1, 0.0, 1.0, 0,0,0]))
        fragmentedBehavior = constructConnectedTraj(surfaceBSpline,state_names,state_vals,temp_masks,corrections,self._time_sampling)
        return temp_masks,trajs,fragmentedBehavior

    def get_uvs(self):
        n_passes = self._n_passes
        horizontal = self._horizontal
        marginX = 0.01
        marginY = 0.05
        length = 1-2*marginY
        
        us=[marginX]
        vs=[marginY]

        for i in range(n_passes):
            x = 1-marginX
            y = marginY+length*i/(n_passes-1)
            us.append(self.linearization(x))
            vs.append(self.linearization(y))
            if i != (n_passes-1):
                x = marginX 
                y = marginY+length*(i+1)/(n_passes-1)
                us.append(self.linearization(x))
                vs.append(self.linearization(y))
        print("u",us)
        print("v",vs)
        if horizontal:
            return vs,us
        return us,vs

    def get_path(self):
        surfaceBSpline = self._surfaceBSpline
        print("getting uvs")
        us,vs = self.get_uvs()
        
        print("getting segments")
        self._mask,self._trajs, self._learnedSegments = self.get_segments(surfaceBSpline,us,vs)
        
        print("done")
        path = Path()
        path.header.frame_id = REFERENCE_FRAME
        for i,segment in enumerate(self._mask):
            for j,val in enumerate(segment):
                if j%self._downsamp == 0 and  val:
                    pose = PoseStamped()
                    pose.header.frame_id = REFERENCE_FRAME

                    pose.pose.position.x = self._trajs[i][0,j]
                    pose.pose.position.y = self._trajs[i][1,j]
                    pose.pose.position.z = self._trajs[i][2,j]
                    pose.pose.orientation.x = self._trajs[i][3,j]
                    pose.pose.orientation.y = self._trajs[i][4,j]
                    pose.pose.orientation.z = self._trajs[i][5,j]
                    pose.pose.orientation.w = self._trajs[i][6,j]
                    path.poses.append(pose)

        return path

    def get_pixel_from_pose(self, pose):
        if pose.header.frame_id != CAMERA_FRAME:
            pose = self._tfBuffer.transform(pose,CAMERA_FRAME)
            
        p = pose.pose.position
        if p.z < 0:
            return None
        p=[p.x,p.y,p.z]
        return self._camera_model.project3dToPixel(p)
    
    def run(self):
        rospy.spin()

    def signal_handler(self, signal, frame):
        sys.exit()

if __name__ == "__main__":
    rospy.init_node('spliner')
    spliner = Spliner()
    signal.signal(signal.SIGINT, spliner.signal_handler)
    spliner.run()