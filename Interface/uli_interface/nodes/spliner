#!/usr/bin/env python3
import numpy as np
import signal
import matplotlib.pyplot as plt
from geometry_msgs.msg import Twist
import time
from pynput import keyboard
import PyKDL
import hmac
import hashlib
import base64
import secrets
import ctypes
import struct
from core_robotics.PyBSpline import convertToUV, BSplineSurface
from corrective_shared_autonomy.TaskModels.DMPLWRhardcoded import HybridSegment, DMPLWRhardcoded
from scipy.spatial.transform import Rotation as ScipyR

import sys
import json

import rospy
from geometry_msgs.msg import Twist
import numpy as np
import rospkg
import ros_numpy
import sensor_msgs.point_cloud2 as pc2
from sensor_msgs.msg import Image, PointCloud2, CameraInfo, JointState, CompressedImage,PointCloud2
from geometry_msgs.msg import TransformStamped, PoseStamped, Point, Pose, PoseArray, Quaternion, Twist, Vector3
from std_msgs.msg import String
from nav_msgs.msg import Path
from scipy.spatial.transform import Slerp

import image_geometry
from tf.transformations import quaternion_from_euler
import tf
import tf2_ros
import tf2_geometry_msgs
from scipy.spatial.transform import Rotation as ScipyR

REFERENCE_FRAME='panda_link0'
CAMERA_FRAME='rgb_camera_link'

# starting vals is a list of starting vals
# ending vals is a list of ending vals
# num_pts is how many interp values
# quat_vars is the index of any starts of quaternions so they can be SLERPed instead of LERPed
# super_pos_vars is the starting index for 2-pairs of variables which should have an added sine wave (e.g., rubbing)
# super_pos_freq is the frequency for the starting index pairs
# super_pos_amp is the amplitude for the starting index pairs
def interpMultD(starting_vals,ending_vals,num_pts,quat_vars=[3], super_pos_vars = [], super_pos_freq=[], super_pos_amp = []):
    vals = []
    # LERP ALL VARIABLES
    for ii in range(0,num_pts):
        # super is to potentially add a circular motion on top of the LERP motion
        freq_super = 0.0
        amp_super = 0.0
        added_sine = np.zeros((np.shape(starting_vals)))
        for jj in range(0,len(super_pos_vars)):
            amp = super_pos_amp[jj]
            freq = super_pos_freq[jj]
            added_sine[super_pos_vars[jj]] = np.sin(np.pi*float(ii)/num_pts)*amp*np.sin(freq*float(ii)/2*np.pi)
            added_sine[super_pos_vars[jj]+1] = np.sin(np.pi*float(ii)/num_pts)*amp*np.cos(freq*float(ii)/2*np.pi)
        c_i = float(ii)/float(num_pts) # interpolation coefficient
        vals.append(list(c_i*(e-s)+s+a for s,e,a in zip(starting_vals,ending_vals,added_sine)))
    vals = np.asarray(vals) # currently,num_samples x num_vars
    # SLERP QUATERIONS
    for jj in range(0,len(quat_vars)):
        starting_q = np.array(starting_vals[quat_vars[jj]:quat_vars[jj]+4])
        ending_q = np.array(ending_vals[quat_vars[jj]:quat_vars[jj]+4])
        key_times = [0,num_pts-1]
        key_rots = ScipyR.from_quat(np.concatenate((starting_q.reshape((1,4)),ending_q.reshape((1,4))),axis=0))
        slerper = Slerp(key_times, key_rots)
        pts = list(range(0,num_pts))
        slerped = slerper(pts)
        vals[:,quat_vars[jj]:quat_vars[jj]+4] = np.asarray(slerped.as_quat())
    return vals.T


def get_mid_points(p1,p2,k):
    return [[p1[0]+i*(p2[0]-p1[0])/(k-1),p1[1]+i*(p2[1]-p1[1])/(k-1)] for i in range(k)]

def get_distance(p1,p2):
    return np.sqrt((p2[0]-p1[0])**2+(p2[1]-p1[1])**2+(p2[2]-p1[2])**2)

class Spliner(object):
    def __init__(self):
        self._tfBuffer = tf2_ros.Buffer()
        self._tl = tf2_ros.TransformListener(self._tfBuffer)
        self._camera_model = None
        # self._model = image_geometry.PinholeCameraModel()
        # camInfo = CameraInfo()
        # camInfo.header.frame_id="camera1"
        # camInfo.height = 1536
        # camInfo.width = 2048
        # camInfo.distortion_model = "rational_polynomial"
        # camInfo.D = [0.5248579382896423, -2.5943498611450195, 0.0008818571805022657, -0.000306136003928259, 1.4509135484695435, 0.4030783474445343, -2.42022705078125, 1.3811763525009155]
        # camInfo.K = [976.9754638671875, 0.0, 1018.8711547851562, 0.0, 976.9026489257812, 780.8445434570312, 0.0, 0.0, 1.0]
        # camInfo.R = [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]
        # camInfo.P = [976.9754638671875, 0.0, 1018.8711547851562, 0.0, 0.0, 976.9026489257812, 780.8445434570312, 0.0, 0.0, 0.0, 1.0, 0.0]
        # self._model.fromCameraInfo(camInfo)

        # self._static_br = tf2_ros.StaticTransformBroadcaster()
        # self._br =tf2_ros.TransformBroadcaster()
        # static_transformStamped = TransformStamped()

        # static_transformStamped.header.stamp = rospy.Time.now()
        # static_transformStamped.header.frame_id = REFERENCE_FRAME
        # static_transformStamped.child_frame_id = "camera_base"

        # static_transformStamped.transform.translation.x = 0
        # static_transformStamped.transform.translation.y = 0
        # static_transformStamped.transform.translation.z = 1

        # quat = tf.transformations.quaternion_from_euler(np.pi,np.pi/2,0)
        # static_transformStamped.transform.rotation.x = quat[0]
        # static_transformStamped.transform.rotation.y = quat[1]
        # static_transformStamped.transform.rotation.z = quat[2]
        # static_transformStamped.transform.rotation.w = quat[3]
        # self._static_br.sendTransform(static_transformStamped)

        self._depth = None
        self._camera_model = None
        self._learnedSegments = None
        self._path_pub = rospy.Publisher("/path",Path,queue_size = 1)
        self._ui_path_pub = rospy.Publisher("/ui/path",String,queue_size = 1)
        self._events_pub = rospy.Publisher("/interaction_events",String,queue_size = 1)
        self._cam_info_sub = rospy.Subscriber("/k4a/depth_to_rgb/camera_info", CameraInfo, self.on_info)

        self._ui_param_sub = rospy.Subscriber("/ui/parameters", String, self.on_param)
        self._n_passes = 2
        self._horizontal = True

    def on_param(self,msg):
        self._param = json.loads(msg.data)
        print(self._param)
        passes_idx = next((i for i, item in enumerate(self._param) if item["label"] == "Number of passes"), None)
        if passes_idx is not None:
            self._n_passes = int(self._param[passes_idx]["value"])

        orientation_idx = next((i for i, item in enumerate(self._param) if item["label"] == "Orientation"), None)
        if orientation_idx is not None:
            self._horizontal = self._param[orientation_idx]["value"] == "horizontal"

    def on_info(self,msg):
        if self._camera_model is None:
            self._width = msg.width
            self._height = msg.height
            self._camera_model = image_geometry.PinholeCameraModel()
            self._camera_model.fromCameraInfo(msg)
            self._depth_sub = rospy.Subscriber("/k4a/depth_to_rgb/image_raw", Image, self.on_depth)
            self._corners_sub = rospy.Subscriber("/ui/commands",String, self.on_command)
        
    def on_command(self,msg):
        l = msg.data.split(":")
        if l[0] == "spline":
            print("splining")
            path = self.get_path(l[1])

            ratio_points = []
            for p in path.poses:
                coord = self.get_pixel_from_pose(p)
                coord=[str(coord[0]/self._width),str(coord[1]/self._height)]
                ratio_points.append(','.join(coord))
            
            self._ui_path_pub.publish(String(';'.join(ratio_points)))
            self._path_pub.publish(path)
            print("splining finished")
        if l[0] == "execute":
            if self._model is not None and self._learnedSegments is not None:
                trans = self._tfBuffer.lookup_transform(REFERENCE_FRAME,CAMERA_FRAME, rospy.Time())

                q=trans.transform.rotation
                t=trans.transform.translation
                R_surface=[q.x,q.y,q.z,q.w]
                t_surface=[t.x,t.y,t.z]

                self._model.executeModel(learnedSegments=self._learnedSegments,R_surface=R_surface,t_surface=t_surface)
                self._events_pub.publish("motion_finished")

            
    def get_xyz_from_pixel(self, x,y,d):
        
        p = [x,y]
        p = self._camera_model.rectifyPoint(p)
        p3d = self._camera_model.projectPixelTo3dRay(p)
        point = Point()
        point.x=d/p3d[2]*p3d[0]
        point.y=d/p3d[2]*p3d[1]
        point.z=d/p3d[2]*p3d[2]
        return point

    def get_close_point(self, x,y):
        dist = 1
        while(True):
            for i in range(-dist,dist):
                if self._depth[x+i,y-dist] > 0:
                    return x+i,y-dist,self._depth[y-dist,x+i]
                if self._depth[x+i,y+dist] > 0:
                    return x+i,y+dist,self._depth[y+dist,x+i]
                if self._depth[x+dist,y+i] > 0:
                    return x+dist,y+i,self._depth[y+i,x+dist]
                if self._depth[x-dist,y+i] > 0:
                    return x-dist,y+i,self._depth[y+i,x-dist]
            dist+=1

    def get_pose_from_pixel(self, x,y,frame_id=REFERENCE_FRAME):
        x = int(x)
        y=int(y)
        d = self._depth[y,x]/1000.
        if not d > 0:
            x,y,d = self.get_close_point(x,y)
            
        point = self.get_xyz_from_pixel(x,y,d)
        p = PoseStamped()
        p.header.frame_id = CAMERA_FRAME
        p.header.stamp = rospy.Time(0)
        p.pose.position = point
        p.pose.orientation.w = 1
        return self._tfBuffer.transform(p,frame_id)

    def get_point(self,p,frame_id = REFERENCE_FRAME):
        p = self.get_pose_from_pixel(p[0],p[1], frame_id)
        if p is None:
            return
        return [p.pose.position.x, p.pose.position.y,p.pose.position.z]

    def on_depth(self, msg):
        self._depth = ros_numpy.numpify(msg)


    
    def get_surface(self,coord):
        #s="484,436_615,436_615,816_484,816"

        ps = coord.split(";")

        p1 = np.array([int(float(ps[0].split(',')[0])*self._width),int(float(ps[0].split(',')[1])*self._height)])
        p2 = np.array([int(float(ps[1].split(',')[0])*self._width),int(float(ps[1].split(',')[1])*self._height)])
        p3 = np.array([int(float(ps[2].split(',')[0])*self._width),int(float(ps[2].split(',')[1])*self._height)])
        p4 = np.array([int(float(ps[3].split(',')[0])*self._width),int(float(ps[3].split(',')[1])*self._height)])
        points = [p1,p2,p3,p4]
        dp = np.array([np.linalg.norm(p) for p in points])
        start_index = np.argmin(dp)
        if points[(start_index + 1)%4][1] > points[start_index-1][1]:
            pp1 = points[start_index]
            pp2 = points[(start_index + 1)%4]
            pp3 = points[(start_index + 2)%4]
            pp4 = points[(start_index + 3)%4]
        else:
            pp1 = points[start_index]
            pp2 = points[start_index - 1]
            pp3 = points[start_index - 2]
            pp4 = points[start_index - 3]
        print("p1 "+str(p1))
        print("p2 "+str(p2))
        print("p3 "+str(p3))
        print("p4 "+str(p4))
        print("pp1 "+str(pp1))
        print("pp2 "+str(pp2))
        print("pp3 "+str(pp3))
        print("pp4 "+str(pp4))
        pose1 = self.get_point(pp1,CAMERA_FRAME)
        pose2 = self.get_point(pp2,CAMERA_FRAME)
        pose3 = self.get_point(pp3,CAMERA_FRAME)
        pose4 = self.get_point(pp4,CAMERA_FRAME)
        
        dt = get_distance(pose1,pose2)
        dr = get_distance(pose2,pose3)
        db = get_distance(pose3,pose4)
        dl = get_distance(pose4,pose1)


        # spline_step = .03
        
        step_x = 10#int(max(dt,db)/spline_step)+1
        step_y = 10#int(max(dr,dl)/spline_step)+1

        top = get_mid_points(pp1,pp2,step_x)
        bot = get_mid_points(pp4,pp3,step_x)
        
        p=[]
        for i in range(step_x):
            p = p + get_mid_points(top[i],bot[i],step_y)
        p = [self.get_point(k,CAMERA_FRAME) for k in p]

        ctrl_p = np.array(p).reshape((step_x,step_y,3))

        surfaceBSpline = BSplineSurface()
        surfaceBSpline.initialize(k=3, control_pts=ctrl_p)

        return surfaceBSpline

    
    def get_approach(self, surfaceBSpline, u0, v0):
        ang_relative_to_surface = 0 # degrees
        R_tool_surf = ScipyR.from_euler('z',180,degrees=True)
        q_ts = R_tool_surf.as_quat()

        segment = HybridSegment()
        segment.hybrid = False
        segment.num_samples = 100 # 2 seconds
        segment.state_names = ['x','y','z','qx','qy','qz','qw','valve']
        approach_pt, n_hat, r_u_norm, r_v_norm = surfaceBSpline.calculate_surface_point(u0, v0)
        R_surf = ScipyR.from_matrix(np.hstack([r_u_norm.reshape((3,1)), r_v_norm.reshape((3,1)), n_hat.reshape((3,1))]))
        q_app = (R_surf * R_tool_surf).as_quat()

        #dot product between robot-approach point
        trans = self._tfBuffer.lookup_transform(REFERENCE_FRAME,CAMERA_FRAME, rospy.Time()).transform.translation  
        view_v = np.array(approach_pt - [trans.x, trans.y, trans.z])
        # if np.dot(view_v,n_hat) < 0:
        #     n_hat = -n_hat

        offset1 =  0.05 * n_hat
        offset2 =  0.005 * n_hat
        starting = [approach_pt[0] + offset1[0], approach_pt[1] + offset1[1], approach_pt[2]  + offset1[2], q_app[0], q_app[1], q_app[2], q_app[3],0]
        ending = [approach_pt[0] + offset2[0], approach_pt[1] + offset2[1], approach_pt[2]  + offset2[2], q_app[0], q_app[1], q_app[2], q_app[3],1]
        orig_vals = interpMultD(starting,ending,segment.num_samples)
        segment.original_vals = []
        segment.original_vals.append(orig_vals)
        #segments.append(segment)
        return segment

    def get_segments(self, surfaceBSpline,us,vs):
        ang_relative_to_surface = 0 # degrees
        R_tool_surf = ScipyR.from_euler('z',180,degrees=True)
        q_ts = R_tool_surf.as_quat()

        segment = HybridSegment()
        segment.hybrid = True
        segment.surface = surfaceBSpline
        segment.state_names = ['u','v','f','theta_qx','theta_qy','theta_qz','theta_qw','valve']
        segment.original_vals = []
        samps = 400
        segment.num_samples = samps *  (len(us)-1)

        segments = []
        for i in range(len(us)-1):
            segments.append(interpMultD([us[i], vs[i], -5.0, q_ts[0], q_ts[1], q_ts[2], q_ts[3],1],[us[i+1], vs[i+1], -5.0, q_ts[0], q_ts[1], q_ts[2], q_ts[3],1],samps))
        segment.original_vals.append(np.hstack(segments))
        return segment

    def get_return(self,surfaceBSpline,un,vn):
        ang_relative_to_surface = 0 # degrees
        R_tool_surf = ScipyR.from_euler('z',180,degrees=True)
        q_ts = R_tool_surf.as_quat()

        segment = HybridSegment()
        segment.hybrid = False
        segment.num_samples = 100 # 2 seconds
        segment.state_names = ['x','y','z','qx','qy','qz','qw','valve']
        segment.original_vals = []

        retract_pt, n_hat, r_u_norm, r_v_norm = surfaceBSpline.calculate_surface_point(un, vn)

        #dot product between robot-retract_pt
        # trans = self._tfBuffer.lookup_transform(REFERENCE_FRAME,CAMERA_FRAME, rospy.Time()).transform.translation  
        # view_v = np.array(retract_pt - [trans.x, trans.y, trans.z])
        # if np.dot(view_v,n_hat) < 0:
        #     n_hat = -n_hat

        trans = self._tfBuffer.lookup_transform(CAMERA_FRAME, "panda_ee", rospy.Time()).transform

        R_surf = ScipyR.from_matrix(np.hstack([r_u_norm.reshape((3,1)), r_v_norm.reshape((3,1)), n_hat.reshape((3,1))]))
        q_ret = (R_surf * R_tool_surf).as_quat()
        offset1 =  0.005 * n_hat
        starting = [retract_pt[0]+offset1[0], retract_pt[1]+offset1[1], retract_pt[2]+offset1[2], q_ret[0], q_ret[1], q_ret[2], q_ret[3],1]
        
        offset2 =  0.05 * n_hat
        # ending = [retract_pt[0]+offset2[0], retract_pt[1]+offset2[1], retract_pt[2]+offset2[2], q_ret[0], q_ret[1], q_ret[2], q_ret[3],0]
        ending = [trans.translation.x, trans.translation.y, trans.translation.z, trans.rotation.x, trans.rotation.y, trans.rotation.z, trans.rotation.w, 0]
        orig_vals = interpMultD(starting,ending,segment.num_samples)
        segment.original_vals.append(orig_vals)
        return segment

    def get_uvs(self):
        n_passes = self._n_passes
        horizontal = self._horizontal
        margin = 0.1
        length = 1-2*margin
        us=[margin]
        vs=[margin]
        for i in range(n_passes):
            x = margin if i%2 else (1-margin)
            y = margin+length*i/(n_passes-1)
            val = [x,y]
            if horizontal:
                val = [y,x]
            us.append(val[0])
            vs.append(val[1])
            if i != (n_passes-1):
                x = margin if i%2 else (1-margin)
                y = margin+length*(i+1)/(n_passes-1)
                val = [x,y]
                if horizontal:
                    val = [y,x]
                us.append(val[0])
                vs.append(val[1])
                
        return us,vs

    def get_path(self, coord):
        surfaceBSpline = self.get_surface(coord)
        print("getting uvs")
        us,vs = self.get_uvs()
        
        print("getting segments")
        segments = [self.get_approach(surfaceBSpline, us[0],vs[0]), self.get_segments(surfaceBSpline,us,vs), self.get_return(surfaceBSpline,us[-1],vs[-1])]
        segments = np.hstack(segments)
        
        print("creating model")
        self._model = DMPLWRhardcoded(verbose=False)
        print("learning")
        self._learnedSegments = self._model.learnModel(segments,'')
        print("getting traj")
        xs,ys,zs = self._model.getTraj(self._learnedSegments,orig=False)
        print("done")
        path = Path()
        path.header.frame_id = CAMERA_FRAME
        for x,y,z in zip(xs,ys,zs):
            
            pose = PoseStamped()
            pose.header.frame_id = CAMERA_FRAME

            pose.pose.position.x = x
            pose.pose.position.y = y
            pose.pose.position.z = z
            pose.pose.orientation.w = 1
            path.poses.append(pose)

        return path

    def get_pixel_from_pose(self, pose):
        if pose.header.frame_id != CAMERA_FRAME:
            pose = self._tfBuffer.transform(pose,CAMERA_FRAME)
            
        p = pose.pose.position
        if p.z < 0:
            return None
        p=[p.x,p.y,p.z]
        return self._camera_model.project3dToPixel(p)
    
    def run(self):
        rospy.spin()

    def signal_handler(self, signal, frame):
        sys.exit()

if __name__ == "__main__":
    rospy.init_node('spliner')
    spliner = Spliner()
    signal.signal(signal.SIGINT, spliner.signal_handler)
    spliner.run()