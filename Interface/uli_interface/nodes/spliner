#!/usr/bin/env python3
import numpy as np
import signal
import matplotlib.pyplot as plt
from geometry_msgs.msg import Twist
import time
from pynput import keyboard
import PyKDL
import hmac
import hashlib
import base64
import secrets
import ctypes
import struct
from core_robotics.PyBSpline import convertToUV, BSplineSurface
from corrective_shared_autonomy.TaskModels.DMPLWRhardcoded import HybridSegment, DMPLWRhardcoded
from scipy.spatial.transform import Rotation as ScipyR

import sys
import json

import rospy
from geometry_msgs.msg import Twist
import numpy as np
import rospkg
import ros_numpy
import sensor_msgs.point_cloud2 as pc2
from sensor_msgs.msg import Image, PointCloud2, CameraInfo, JointState, CompressedImage,PointCloud2
from geometry_msgs.msg import TransformStamped, PoseStamped, Point, Pose, PoseArray, Quaternion, Twist, Vector3
from std_msgs.msg import String
from nav_msgs.msg import Path

import image_geometry
from tf.transformations import quaternion_from_euler
import tf
import tf2_ros
import tf2_geometry_msgs
from scipy.spatial.transform import Rotation as ScipyR

REFERENCE_FRAME='panda_link0'
CAMERA_FRAME='rgb_camera_link'

def interpMultD(starting_vals,ending_vals,num_pts):
    vals = []
    for ii in range(0,num_pts):
        c_i = float(ii)/float(num_pts) # interpolation coefficient
        vals.append(list(c_i*(e-s)+s for s,e in zip(starting_vals,ending_vals)))
    return np.asarray(vals).T


def get_mid_points(p1,p2,k):
    return [[p1[0]+i*(p2[0]-p1[0])/(k-1),p1[1]+i*(p2[1]-p1[1])/(k-1)] for i in range(k)]

def get_distance(p1,p2):
    return np.sqrt((p2[0]-p1[0])**2+(p2[1]-p1[1])**2+(p2[2]-p1[2])**2)

class Spliner(object):
    def __init__(self):
        self._tfBuffer = tf2_ros.Buffer()
        self._tl = tf2_ros.TransformListener(self._tfBuffer)
        self._camera_model = None
        # self._model = image_geometry.PinholeCameraModel()
        # camInfo = CameraInfo()
        # camInfo.header.frame_id="camera1"
        # camInfo.height = 1536
        # camInfo.width = 2048
        # camInfo.distortion_model = "rational_polynomial"
        # camInfo.D = [0.5248579382896423, -2.5943498611450195, 0.0008818571805022657, -0.000306136003928259, 1.4509135484695435, 0.4030783474445343, -2.42022705078125, 1.3811763525009155]
        # camInfo.K = [976.9754638671875, 0.0, 1018.8711547851562, 0.0, 976.9026489257812, 780.8445434570312, 0.0, 0.0, 1.0]
        # camInfo.R = [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]
        # camInfo.P = [976.9754638671875, 0.0, 1018.8711547851562, 0.0, 0.0, 976.9026489257812, 780.8445434570312, 0.0, 0.0, 0.0, 1.0, 0.0]
        # self._model.fromCameraInfo(camInfo)

        # self._static_br = tf2_ros.StaticTransformBroadcaster()
        # self._br =tf2_ros.TransformBroadcaster()
        # static_transformStamped = TransformStamped()

        # static_transformStamped.header.stamp = rospy.Time.now()
        # static_transformStamped.header.frame_id = REFERENCE_FRAME
        # static_transformStamped.child_frame_id = "camera_base"

        # static_transformStamped.transform.translation.x = 0
        # static_transformStamped.transform.translation.y = 0
        # static_transformStamped.transform.translation.z = 1

        # quat = tf.transformations.quaternion_from_euler(np.pi,np.pi/2,0)
        # static_transformStamped.transform.rotation.x = quat[0]
        # static_transformStamped.transform.rotation.y = quat[1]
        # static_transformStamped.transform.rotation.z = quat[2]
        # static_transformStamped.transform.rotation.w = quat[3]
        # self._static_br.sendTransform(static_transformStamped)

        self._depth = None
        self._camera_model = None
        self._learnedSegments = None
        self._path_pub = rospy.Publisher("/path",Path,queue_size = 1)
        self._ui_path_pub = rospy.Publisher("/ui/path",String,queue_size = 1)
        self._cam_info_sub = rospy.Subscriber("/depth_to_rgb/camera_info", CameraInfo, self.on_info)

        self._ui_param_sub = rospy.Subscriber("/ui/parameters", String, self.on_param)
        self._n_passes = 2
        self._horizontal = True

    def on_param(self,msg):
        self._param = json.loads(msg.data)
        print(self._param)
        passes_idx = next((i for i, item in enumerate(self._param) if item["label"] == "Number of passes"), None)
        if passes_idx is not None:
            self._n_passes = int(self._param[passes_idx]["value"])

        orientation_idx = next((i for i, item in enumerate(self._param) if item["label"] == "Orientation"), None)
        if orientation_idx is not None:
            self._horizontal = self._param[orientation_idx]["value"] == "horizontal"

    def on_info(self,msg):
        if self._camera_model is None:
            self._width = msg.width
            self._height = msg.height
            self._camera_model = image_geometry.PinholeCameraModel()
            self._camera_model.fromCameraInfo(msg)
            self._depth_sub = rospy.Subscriber("/depth_to_rgb/image_raw", Image, self.on_depth)
            self._corners_sub = rospy.Subscriber("/ui/commands",String, self.on_command)
        
    def on_command(self,msg):
        l = msg.data.split(":")
        if l[0] == "spline":
            print("splining")
            path = self.get_path(l[1])

            ratio_points = []
            for p in path.poses:
                coord = self.get_pixel_from_pose(p)
                coord=[str(coord[0]/self._width),str(coord[1]/self._height)]
                ratio_points.append(','.join(coord))
            
            self._ui_path_pub.publish(String(';'.join(ratio_points)))
            self._path_pub.publish(path)
            print("splining finished")
        if l[0] == "execute":
            if self._model is not None and self._learnedSegments is not None:
                trans = self._tfBuffer.lookup_transform(REFERENCE_FRAME,CAMERA_FRAME, rospy.Time())

                q=trans.transform.rotation
                t=trans.transform.translation
                R_surface=[q.x,q.y,q.z,q.w]
                t_surface=[t.x,t.y,t.z]

                self._model.executeModel(learnedSegments=self._learnedSegments,R_surface=R_surface,t_surface=t_surface)

            
    def get_xyz_from_pixel(self, x,y,d):
        
        p = [x,y]
        p = self._camera_model.rectifyPoint(p)
        p3d = self._camera_model.projectPixelTo3dRay(p)
        point = Point()
        point.x=d/p3d[2]*p3d[0]
        point.y=d/p3d[2]*p3d[1]
        point.z=d/p3d[2]*p3d[2]
        return point

    def get_close_point(self, x,y):
        dist = 1
        while(True):
            for i in range(-dist,dist):
                if self._depth[x+i,y-dist] > 0:
                    return x+i,y-dist,self._depth[y-dist,x+i]
                if self._depth[x+i,y+dist] > 0:
                    return x+i,y+dist,self._depth[y+dist,x+i]
                if self._depth[x+dist,y+i] > 0:
                    return x+dist,y+i,self._depth[y+i,x+dist]
                if self._depth[x-dist,y+i] > 0:
                    return x-dist,y+i,self._depth[y+i,x-dist]
            dist+=1

    def get_pose_from_pixel(self, x,y,frame_id=REFERENCE_FRAME):
        x = int(x)
        y=int(y)
        d = self._depth[y,x]/1000.
        if not d > 0:
            x,y,d = self.get_close_point(x,y)
            
        point = self.get_xyz_from_pixel(x,y,d)
        p = PoseStamped()
        p.header.frame_id = CAMERA_FRAME
        p.header.stamp = rospy.Time(0)
        p.pose.position = point
        p.pose.orientation.w = 1
        return self._tfBuffer.transform(p,frame_id)

    def get_point(self,p,frame_id = REFERENCE_FRAME):
        p = self.get_pose_from_pixel(p[0],p[1], frame_id)
        if p is None:
            return
        return [p.pose.position.x, p.pose.position.y,p.pose.position.z]

    def on_depth(self, msg):
        self._depth = ros_numpy.numpify(msg)


    
    def get_surface(self,coord):
        #s="484,436_615,436_615,816_484,816"

        ps = coord.split(";")

        p1 = [int(float(ps[0].split(',')[0])*self._width),int(float(ps[0].split(',')[1])*self._height)]
        p2 = [int(float(ps[1].split(',')[0])*self._width),int(float(ps[1].split(',')[1])*self._height)]
        p3 = [int(float(ps[2].split(',')[0])*self._width),int(float(ps[2].split(',')[1])*self._height)]
        p4 = [int(float(ps[3].split(',')[0])*self._width),int(float(ps[3].split(',')[1])*self._height)]

        pose1 = self.get_point(p1,CAMERA_FRAME)
        pose2 = self.get_point(p2,CAMERA_FRAME)
        pose3 = self.get_point(p3,CAMERA_FRAME)
        pose4 = self.get_point(p4,CAMERA_FRAME)
        
        dt = get_distance(pose1,pose2)
        dr = get_distance(pose2,pose3)
        db = get_distance(pose3,pose4)
        dl = get_distance(pose4,pose1)


        spline_step = .02
        
        step_x = int(max(dt,db)/spline_step)+1
        step_y = int(max(dr,dl)/spline_step)+1

        bot = get_mid_points(p1,p2,step_x)
        top = get_mid_points(p4,p3,step_x)
        
        p=[]
        for i in range(step_x):
            p = p + get_mid_points(top[i],bot[i],step_y)
        p = [self.get_point(k,CAMERA_FRAME) for k in p]

        ctrl_p = np.array(p).reshape((step_x,step_y,3))

        surfaceBSpline = BSplineSurface()
        surfaceBSpline.initialize(k=3, control_pts=ctrl_p)

        return surfaceBSpline

    
    def get_approach(self, surfaceBSpline, u0, v0):
        ang_relative_to_surface = 0 # degrees
        R_tool_surf = ScipyR.from_euler('y',ang_relative_to_surface,degrees=True)
        q_ts = R_tool_surf.as_quat()

        segment = HybridSegment()
        segment.hybrid = False
        segment.num_samples = 100 # 2 seconds
        segment.state_names = ['x','y','z','qx','qy','qz','qw']
        approach_pt, n_hat, r_u_norm, r_v_norm = surfaceBSpline.calculate_surface_point(u0, v0)
        R_surf = ScipyR.from_matrix(np.hstack([r_u_norm.reshape((3,1)), r_v_norm.reshape((3,1)), n_hat.reshape((3,1))]))
        q_app = (R_surf * R_tool_surf).as_quat()

        offset =  0.05 * n_hat
        starting = [approach_pt[0] + offset[0], approach_pt[1] + offset[1], approach_pt[2]  + offset[2], q_app[0], q_app[1], q_app[2], q_app[3]]
        ending = [approach_pt[0], approach_pt[1], approach_pt[2] + 0.0, q_app[0], q_app[1], q_app[2], q_app[3]]
        orig_vals = interpMultD(starting,ending,segment.num_samples)
        segment.original_vals = []
        segment.original_vals.append(orig_vals)
        #segments.append(segment)
        return segment

    def get_segments(self, surfaceBSpline,us,vs):
        ang_relative_to_surface = 0 # degrees
        R_tool_surf = ScipyR.from_euler('y',ang_relative_to_surface,degrees=True)
        q_ts = R_tool_surf.as_quat()

        segment = HybridSegment()
        segment.hybrid = True
        segment.surface = surfaceBSpline
        segment.state_names = ['u','v','f','theta_qx','theta_qy','theta_qz','theta_qw']
        segment.original_vals = []
        samps = 150
        segment.num_samples = samps *  (len(us)-1)

        segments = []
        for i in range(len(us)-1):
            segments.append(interpMultD([us[i], vs[i], -5.0, q_ts[0], q_ts[1], q_ts[2], q_ts[3]],[us[i+1], vs[i+1], -5.0, q_ts[0], q_ts[1], q_ts[2], q_ts[3]],samps))
        segment.original_vals.append(np.hstack(segments))
        return segment

    def get_return(self,surfaceBSpline,un,vn):
        ang_relative_to_surface = 0 # degrees
        R_tool_surf = ScipyR.from_euler('y',ang_relative_to_surface,degrees=True)
        q_ts = R_tool_surf.as_quat()

        segment = HybridSegment()
        segment.hybrid = False
        segment.num_samples = 100 # 2 seconds
        segment.state_names = ['x','y','z','qx','qy','qz','qw']
        segment.original_vals = []

        retract_pt, n_hat, r_u_norm, r_v_norm = surfaceBSpline.calculate_surface_point(un, vn)
        offset =  0.05 * n_hat
        R_surf = ScipyR.from_matrix(np.hstack([r_u_norm.reshape((3,1)), r_v_norm.reshape((3,1)), n_hat.reshape((3,1))]))
        q_ret = (R_surf * R_tool_surf).as_quat()
        offset1 =  0.005 * n_hat
        starting = [retract_pt[0]+offset1[0], retract_pt[1]+offset1[1], retract_pt[2]+offset1[2], q_ret[0], q_ret[1], q_ret[2], q_ret[3]]
        offset2 =  0.05 * n_hat
        ending = [retract_pt[0]+offset2[0], retract_pt[1]+offset2[1], retract_pt[2]+offset2[2], q_ret[0], q_ret[1], q_ret[2], q_ret[3]]
        orig_vals = interpMultD(starting,ending,segment.num_samples)
        segment.original_vals.append(orig_vals)
        return segment

    def get_uvs(self):
        n_passes = self._n_passes
        horizontal = self._horizontal
        margin = 0.1
        length = 1-2*margin
        us=[margin]
        vs=[margin]
        for i in range(n_passes):
            x = margin if i%2 else (1-margin)
            y = margin+length*i/n_passes
            val = [x,y]
            if not horizontal:
                val = [y,x]
            us.append(val[0])
            vs.append(val[1])
            if i != (n_passes-1):
                x = margin if i%2 else (1-margin)
                y = margin+length*(i+1)/n_passes
                val = [x,y]
                if not horizontal:
                    val = [y,x]
                us.append(val[0])
                vs.append(val[1])
                
        return us,vs

    def get_path(self, coord):
        surfaceBSpline = self.get_surface(coord)
        us,vs = self.get_uvs()
        
        segments = [self.get_approach(surfaceBSpline, us[0],vs[0]), self.get_segments(surfaceBSpline,us,vs), self.get_return(surfaceBSpline,us[-1],vs[-1])]
        segments = np.hstack(segments)
        
        self._model = DMPLWRhardcoded(verbose=False)
        self._learnedSegments = self._model.learnModel(segments,'')
        xs,ys,zs = self._model.getTraj(self._learnedSegments,orig=False)
        
        path = Path()
        path.header.frame_id = CAMERA_FRAME
        for x,y,z in zip(xs,ys,zs):
            
            pose = PoseStamped()
            pose.header.frame_id = CAMERA_FRAME

            pose.pose.position.x = x
            pose.pose.position.y = y
            pose.pose.position.z = z
            pose.pose.orientation.w = 1
            path.poses.append(pose)

        return path

    def get_pixel_from_pose(self, pose):
        if pose.header.frame_id != CAMERA_FRAME:
            pose = self._tfBuffer.transform(pose,CAMERA_FRAME)
            
        p = pose.pose.position
        if p.z < 0:
            return None
        p=[p.x,p.y,p.z]
        return self._camera_model.project3dToPixel(p)
    
    def run(self):
        rospy.spin()

    def signal_handler(self, signal, frame):
        sys.exit()

if __name__ == "__main__":
    rospy.init_node('spliner')
    spliner = Spliner()
    signal.signal(signal.SIGINT, spliner.signal_handler)
    spliner.run()