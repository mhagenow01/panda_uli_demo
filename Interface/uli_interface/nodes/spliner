#!/usr/bin/env python3
from random import sample
import numpy as np
import signal
import matplotlib.pyplot as plt
from geometry_msgs.msg import Twist
import time
from pynput import keyboard
import PyKDL
import hmac
import hashlib
import base64
import secrets
import ctypes
import struct
from core_robotics.PyBSpline import convertToUV, BSplineSurface
from corrective_shared_autonomy.TaskModels.DMPLWRhardcoded import HybridSegment, DMPLWRhardcoded
from scipy.spatial.transform import Rotation as ScipyR
from corrective_shared_autonomy.TaskModels.FragmentedExecution import getFragmentedTraj, constructConnectedTraj
import cv2
from threading import Thread

from cv_bridge import CvBridge

import sys
import json

import rospy
from geometry_msgs.msg import Twist
import numpy as np
import rospkg
import ros_numpy
import sensor_msgs.point_cloud2 as pc2
from sensor_msgs.msg import Image, PointCloud2, CameraInfo, JointState, CompressedImage,PointCloud2
from geometry_msgs.msg import TransformStamped, PoseStamped, Point, Pose, PoseArray, Quaternion, Twist, Vector3, PointStamped
from std_msgs.msg import String
from nav_msgs.msg import Path
from scipy.spatial.transform import Slerp

import image_geometry
from tf.transformations import quaternion_from_euler
import tf
import tf2_ros
import ctypes
import threading

import tf2_geometry_msgs
from scipy.spatial.transform import Rotation as ScipyR

from franka_core_msgs.msg import RobotState

from corrective_shared_autonomy.TaskModels.FragmentedExecution import checkReachabilityOfPoses

REFERENCE_FRAME='panda_link0'
CAMERA_FRAME='rgb_camera_link'

def get_mid_points(p1,p2,k):
    return [[p1[0]+i*(p2[0]-p1[0])/(k-1),p1[1]+i*(p2[1]-p1[1])/(k-1)] for i in range(k)]

def get_distance(p1,p2):
    return np.sqrt((p2[0]-p1[0])**2+(p2[1]-p1[1])**2+(p2[2]-p1[2])**2)

# https://www.geeksforgeeks.org/python-different-ways-to-kill-a-thread/
class thread_with_trace(threading.Thread):
  def __init__(self, *args, **keywords):
    threading.Thread.__init__(self, *args, **keywords)
    self.killed = False
 
  def start(self):
    self.__run_backup = self.run
    self.run = self.__run     
    threading.Thread.start(self)
 
  def __run(self):
    sys.settrace(self.globaltrace)
    self.__run_backup()
    self.run = self.__run_backup
 
  def globaltrace(self, frame, event, arg):
    if event == 'call':
      return self.localtrace
    else:
      return None
 
  def localtrace(self, frame, event, arg):
    if self.killed:
      if event == 'line':
        raise SystemExit()
    return self.localtrace
 
  def kill(self):
    self.killed = True

class Spliner(object):
    def __init__(self):
        self._tfBuffer = tf2_ros.Buffer()
        self._tl = tf2_ros.TransformListener(self._tfBuffer)
        self._camera_model = None
        self._bridge = CvBridge()
        self._depth_stale = True
        # self._model = image_geometry.PinholeCameraModel()
        # camInfo = CameraInfo()
        # camInfo.header.frame_id="camera1"
        # camInfo.height = 1536
        # camInfo.width = 2048
        # camInfo.distortion_model = "rational_polynomial"
        # camInfo.D = [0.5248579382896423, -2.5943498611450195, 0.0008818571805022657, -0.000306136003928259, 1.4509135484695435, 0.4030783474445343, -2.42022705078125, 1.3811763525009155]
        # camInfo.K = [976.9754638671875, 0.0, 1018.8711547851562, 0.0, 976.9026489257812, 780.8445434570312, 0.0, 0.0, 1.0]
        # camInfo.R = [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]
        # camInfo.P = [976.9754638671875, 0.0, 1018.8711547851562, 0.0, 0.0, 976.9026489257812, 780.8445434570312, 0.0, 0.0, 0.0, 1.0, 0.0]
        # self._model.fromCameraInfo(camInfo)

        # self._static_br = tf2_ros.StaticTransformBroadcaster()
        # self._br =tf2_ros.TransformBroadcaster()
        # static_transformStamped = TransformStamped()

        # static_transformStamped.header.stamp = rospy.Time.now()
        # static_transformStamped.header.frame_id = REFERENCE_FRAME
        # static_transformStamped.child_frame_id = "camera_base"

        # static_transformStamped.transform.translation.x = 0
        # static_transformStamped.transform.translation.y = 0
        # static_transformStamped.transform.translation.z = 1

        # quat = tf.transformations.quaternion_from_euler(np.pi,np.pi/2,0)
        # static_transformStamped.transform.rotation.x = quat[0]
        # static_transformStamped.transform.rotation.y = quat[1]
        # static_transformStamped.transform.rotation.z = quat[2]
        # static_transformStamped.transform.rotation.w = quat[3]
        # self._static_br.sendTransform(static_transformStamped)

        self._depth = None
        self._camera_model = None
        self._learnedSegments = None
        self._scanning = False
        self._path_pub = rospy.Publisher("/path",Path,queue_size = 1)
        self._ui_path_pub = rospy.Publisher("/ui/path",String,queue_size = 1)
        self._ui_feedback_pub = rospy.Publisher("/ui/feedback",String,queue_size = 1)
        self._ui_reach_pub = rospy.Publisher("/ui/reach",String,queue_size = 1)
        self._ui_robot_state_pub = rospy.Publisher("/ui/robot_state",String,queue_size = 1)
        self._events_pub = rospy.Publisher("/interaction_events",String,queue_size = 1)
        self._points_pub = rospy.Publisher("/clicked_point",PointStamped,queue_size = 1)
        self._pose_pub = rospy.Publisher("/checked_poses", PoseArray,queue_size=1)
        self._reach_thread = None

        self._n_passes = 2
        self._horizontal = True
        self._time_sampling = 20.0
        self._pitch = 0

        self._surfaceBSpline = None
        self._robot_state = "grey"

        self._depth_sub = None
        self._corners_sub = None

        self._robot_state_sub = rospy.Subscriber("/franka_ros_interface/custom_franka_state_controller/robot_state", RobotState, self.on_state)
        self._cam_info_sub = rospy.Subscriber("/k4a/depth_to_rgb/camera_info", CameraInfo, self.on_info)
        self._rviz_trigger_sub = rospy.Subscriber("/rviz_triggers", String, self.on_trigger)
        self._ui_param_sub = rospy.Subscriber("/ui/parameters", String, self.on_param)

    def on_trigger(self, msg):
        if msg.data == "scan":
            self._scanning = True
            self._depth_sub.unregister()
            self._corners_sub.unregister()
        if msg.data == "scanningdone":
            self.register_depth()
            

    def on_state(self, msg):
        if msg.robot_mode == 1 or msg.robot_mode ==2: # idle or moving (https://frankaemika.github.io/libfranka/robot__state_8h.html#adfe059ae23ebbad59e421edaa879651a)
            self._robot_state = "green"
        if msg.robot_mode == 4:
            self._robot_state = "red"
        if msg.robot_mode == 5:
            self._robot_state = "orange"
        self._ui_robot_state_pub.publish(self._robot_state)
        
    def get_param(self, label):
        idx = next((i for i, item in enumerate(self._param) if item["id"] == label), None)
        if idx is not None:
            return self._param[idx]["value"]
        else:
            return None

    def on_param(self,msg):
        self._param = json.loads(msg.data)
        print(self._param)
        
        passes = self.get_param("passes")
        if passes is not None:
            self._n_passes = int(passes)
        else:
            self._n_passes = 2

        force = self.get_param("force")
        if force is not None:
            self._force = -int(force)
        else:
            self._force = -5.

        pitch = self.get_param("pitch")
        if pitch is not None:
            self._pitch = int(pitch)
        else:
            self._pitch = 0
        velocity = self.get_param("speed")
        if velocity is not None:
            self._velocity = int(velocity)/1000.
        else:
            self._velocity = .01

        self._horizontal = self.get_param("direction") == "Horizontal"
        self._ui_robot_state_pub.publish(self._robot_state)

    def register_depth(self):
        self._depth_sub = rospy.Subscriber("/k4a/depth_to_rgb/image_raw/compressed", CompressedImage, self.on_depth)
        self._corners_sub = rospy.Subscriber("/ui/commands",String, self.on_command)

    def on_info(self,msg):
        if self._camera_model is None:
            self._width = msg.width
            self._height = msg.height
            self._camera_model = image_geometry.PinholeCameraModel()
            self._camera_model.fromCameraInfo(msg)
            
        if self._depth_sub is None and not self._scanning:
            self.register_depth()
        
    def get_reach(self, p):
        reachable = checkReachabilityOfPoses(p)
        good = []
        bad = []
        for index,reach in enumerate(reachable):
            pose = PoseStamped()
            pose.header.frame_id=REFERENCE_FRAME
            pose.pose.position.x = p[0,index]
            pose.pose.position.y = p[1,index]
            pose.pose.position.z = p[2,index]
            pose.pose.orientation.x = p[3,index]
            pose.pose.orientation.y = p[4,index]
            pose.pose.orientation.z = p[5,index]
            pose.pose.orientation.w = p[6,index]
            # print(pose)
            coord = self.get_pixel_from_pose(pose)
            if coord is None:
                continue
            coord=[str(coord[0]/self._width),str(coord[1]/self._height)]
            if reach:
                good.append(','.join(coord))
            else:
                bad.append(','.join(coord))
        
        self._ui_reach_pub.publish("good:"+';'.join(good))
        rospy.sleep(0.1)
        self._ui_reach_pub.publish("bad:"+';'.join(bad))
        self._reach_thread = None
        self._ui_feedback_pub.publish("4;Reach computed")
    def on_command(self,msg):
        l = msg.data.split(":")
        if l[0] == "stop_reach":
            if self._reach_thread is not None:
                print("killing thread")
                self._reach_thread.kill()
        if l[0] == "spline":
            self._surfaceBSpline = self.get_surface(l[1],REFERENCE_FRAME)
            print("splining finished")
            k=15
            p=np.zeros((7,k*k))
            R_tool_surf = ScipyR.from_euler('z',180,degrees=True)
            try:
                for u in range(k):
                    for v in range(k):
                        x=min(.999,u/float(k-1))
                        y=min(.999,v/float(k-1))
                        pt, n_hat, r_u_norm, r_v_norm = self._surfaceBSpline.calculate_surface_point(x, y)
                        R_surf = ScipyR.from_matrix(np.hstack([r_u_norm.reshape((3,1)), r_v_norm.reshape((3,1)), n_hat.reshape((3,1))]))
                        q = (R_surf * R_tool_surf).as_quat()
                        p[:,u*k+v]=np.array([pt[0],pt[1],pt[2],q[0],q[1],q[2],q[3]])
            except:
                self._ui_feedback_pub.publish("4;Error, out of bounds")
                return
            self._ui_feedback_pub.publish("20;Computing reach")
            ros_poses = PoseArray()
            ros_poses.header.stamp = rospy.Time.now()
            ros_poses.header.frame_id = "panda_link0"

            for pose in np.array(p).T:
                pp = Pose()
                pp.position.x = pose[0]
                pp.position.y = pose[1]
                pp.position.z = pose[2]
                pp.orientation.x = pose[3]
                pp.orientation.y = pose[4]
                pp.orientation.z = pose[5]
                pp.orientation.w = pose[6]
                ros_poses.poses.append(pp)
            
            self._pose_pub.publish(ros_poses)
            rospy.sleep(.01)
            if self._reach_thread is not None:
                self._reach_thread.kill()
            self._reach_thread = thread_with_trace(target = self.get_reach, args=(p,))
            self._reach_thread.start()

        if l[0] == "get_path":
            if self._surfaceBSpline is None:
                return
            self._ui_feedback_pub.publish("40;Computing path")
            path = self.get_path()
            ratio_points = []
            for p in path.poses:
                coord = self.get_pixel_from_pose(p)
                coord=[str(coord[0]/self._width),str(coord[1]/self._height)]
                ratio_points.append(','.join(coord))
            
            self._ui_path_pub.publish(String(';'.join(ratio_points)))
            self._path_pub.publish(path)
            self._ui_feedback_pub.publish("2;Path computed")
            print("path finished")
        if l[0] == "push":
            print("push")
            x = int(float(l[1].split(",")[0])*self._width)
            y = int(float(l[1].split(",")[1])*self._height)
            p = self.get_point([x,y],CAMERA_FRAME)
        if l[0] == "publish_point":
            x = int(float(l[1].split(",")[0])*self._width)
            y = int(float(l[1].split(",")[1])*self._height)
            p = self.get_point([x,y])
            point = PointStamped()
            point.header.frame_id = REFERENCE_FRAME
            point.header.stamp = rospy.Time.now()
            point.point.x = p[0]
            point.point.y = p[1]
            point.point.z = p[2]
            self._points_pub.publish(point)
        if l[0] == "execute":
            if self._learnedSegments is not None:

                self._ui_feedback_pub.publish("2000;Executing")
                model = DMPLWRhardcoded(verbose=True, dt=1.0/self._time_sampling)
                model.executeModel(learnedSegments=self._learnedSegments, input_type='1dof')
            
                self._ui_feedback_pub.publish("4;Finished executing")

                self._events_pub.publish("motion_finished")

            
    def get_xyz_from_pixel(self, x,y,d):
        
        p = [x,y]
        p = self._camera_model.rectifyPoint(p)
        p3d = self._camera_model.projectPixelTo3dRay(p)
        point = Point()
        point.x=d/p3d[2]*p3d[0]
        point.y=d/p3d[2]*p3d[1]
        point.z=d/p3d[2]*p3d[2]
        return point

    def get_close_point(self, x,y):
        dist = 1
        while(True):
            for i in range(-dist,dist):
                if self._depth[x+i,y-dist] > 0:
                    return x+i,y-dist,self._depth[y-dist,x+i]
                if self._depth[x+i,y+dist] > 0:
                    return x+i,y+dist,self._depth[y+dist,x+i]
                if self._depth[x+dist,y+i] > 0:
                    return x+dist,y+i,self._depth[y+i,x+dist]
                if self._depth[x-dist,y+i] > 0:
                    return x-dist,y+i,self._depth[y+i,x-dist]
            dist+=1

    def get_pose_from_pixel(self, x,y,frame_id=REFERENCE_FRAME):
        if self._depth_stale is True:
            self._depth_stale = False
            self._depth = np.asarray(self._bridge.compressed_imgmsg_to_cv2(self._depth_msg))
        x = int(x)
        y=int(y)
        d = self._depth[y,x]/1000.
        if not d > 0:
            x,y,d = self.get_close_point(x,y)
            
        point = self.get_xyz_from_pixel(x,y,d)
        p = PoseStamped()
        p.header.frame_id = CAMERA_FRAME
        p.header.stamp = rospy.Time(0)
        p.pose.position = point
        p.pose.orientation.w = 1
        return self._tfBuffer.transform(p,frame_id)

    def get_point(self,p,frame_id = REFERENCE_FRAME):
        p = self.get_pose_from_pixel(p[0],p[1], frame_id)
        if p is None:
            return
        return [p.pose.position.x, p.pose.position.y,p.pose.position.z]

    def on_depth(self, msg):
        self._depth_msg = msg 
        self._depth_stale = True
    
    def get_surface(self,coord, frame_id = CAMERA_FRAME):
        #s="484,436_615,436_615,816_484,816"

        ps = coord.split(";")

        p1 = np.array([int(float(ps[0].split(',')[0])*self._width),int(float(ps[0].split(',')[1])*self._height)])
        p2 = np.array([int(float(ps[1].split(',')[0])*self._width),int(float(ps[1].split(',')[1])*self._height)])
        p3 = np.array([int(float(ps[2].split(',')[0])*self._width),int(float(ps[2].split(',')[1])*self._height)])
        p4 = np.array([int(float(ps[3].split(',')[0])*self._width),int(float(ps[3].split(',')[1])*self._height)])
        points = [p1,p2,p3,p4]
        dp = np.array([np.linalg.norm(p) for p in points])
        start_index = np.argmin(dp)
        if points[(start_index + 1)%4][1] > points[start_index-1][1]:
            pp1 = points[start_index]
            pp2 = points[(start_index + 1)%4]
            pp3 = points[(start_index + 2)%4]
            pp4 = points[(start_index + 3)%4]
        else:
            pp1 = points[start_index]
            pp2 = points[start_index - 1]
            pp3 = points[start_index - 2]
            pp4 = points[start_index - 3]
        pose1 = self.get_point(pp1,frame_id)
        pose2 = self.get_point(pp2,frame_id)
        pose3 = self.get_point(pp3,frame_id)
        pose4 = self.get_point(pp4,frame_id)
        
        dt = get_distance(pose1,pose2)
        dr = get_distance(pose2,pose3)
        db = get_distance(pose3,pose4)
        dl = get_distance(pose4,pose1)


        # spline_step = .03
        
        step_x = 10#int(max(dt,db)/spline_step)+1
        step_y = 10#int(max(dr,dl)/spline_step)+1

        top = get_mid_points(pp1,pp2,step_x)
        bot = get_mid_points(pp4,pp3,step_x)
        
        p=[]
        for i in range(step_x):
            p = p + get_mid_points(top[i],bot[i],step_y)
        p = [self.get_point(k,frame_id) for k in p]

        ctrl_p = np.array(p).reshape((step_x,step_y,3))

        surfaceBSpline = BSplineSurface()
        surfaceBSpline.initialize(k=3, control_pts=ctrl_p)

        return surfaceBSpline

    
    def get_approach(self, surfaceBSpline, u0, v0):
        ang_relative_to_surface = 0 # degrees
        R_tool_surf = ScipyR.from_euler('z',180,degrees=True)
        q_ts = R_tool_surf.as_quat()

        segment = HybridSegment()
        segment.hybrid = False
        segment.num_samples = 10 # 2 seconds
        segment.state_names = ['x','y','z','qx','qy','qz','qw','valve']
        approach_pt, n_hat, r_u_norm, r_v_norm = surfaceBSpline.calculate_surface_point(u0, v0)
        R_surf = ScipyR.from_matrix(np.hstack([r_u_norm.reshape((3,1)), r_v_norm.reshape((3,1)), n_hat.reshape((3,1))]))
        q_app = (R_surf * R_tool_surf).as_quat()

        #dot product between robot-approach point
        trans = self._tfBuffer.lookup_transform(REFERENCE_FRAME,CAMERA_FRAME, rospy.Time()).transform.translation  
        view_v = np.array(approach_pt - [trans.x, trans.y, trans.z])
        # if np.dot(view_v,n_hat) < 0:
        #     n_hat = -n_hat

        offset1 =  0.05 * n_hat
        offset2 =  0.005 * n_hat
        starting = [approach_pt[0] + offset1[0], approach_pt[1] + offset1[1], approach_pt[2]  + offset1[2], q_app[0], q_app[1], q_app[2], q_app[3],0]
        ending = [approach_pt[0] + offset2[0], approach_pt[1] + offset2[1], approach_pt[2]  + offset2[2], q_app[0], q_app[1], q_app[2], q_app[3],1]
        orig_vals = interpMultD(starting,ending,segment.num_samples)
        
        segment.corrections.append(np.zeros((len(segment.state_names),segment.num_samples)))
        segment.original_vals = []
        segment.original_vals.append(orig_vals)
        #segments.append(segment)
        return segment

    def get_segments(self, surfaceBSpline,us,vs):
        if self._horizontal:
            R_tool_surf = ScipyR.from_euler('zx',[180,self._pitch],degrees=True)
        else:
            R_tool_surf = ScipyR.from_euler('zy',[180,self._pitch],degrees=True)
        q_ts = R_tool_surf.as_quat()

        hybrid = True
        surface = surfaceBSpline
        state_names = ['u','v','f','theta_qx','theta_qy','theta_qz','theta_qw','delta_s','valve']
        original_vals = []
        num_samples = 0
        state_vals = []
        for i in range(int(len(us)/2)):
            start, _, _, _ = self._surfaceBSpline.calculate_surface_point(us[2*i], vs[2*i])
            end, _, _, _ = self._surfaceBSpline.calculate_surface_point(us[2*i+1], vs[2*i+1])
            distance = np.linalg.norm(start-end)
            samps = int(20 * distance / self._velocity)
            state_val = np.zeros((len(state_names),samps)) 
            for j in range(samps):
                state_val[0,j]=us[2*i]+j/samps*(us[2*i+1]-us[2*i])
                state_val[1,j]=vs[2*i]+j/samps*(vs[2*i+1]-vs[2*i])
                state_val[2,j]=self._force
                state_val[3,j]=q_ts[0]
                state_val[4,j]=q_ts[1]
                state_val[5,j]=q_ts[2]
                state_val[6,j]=q_ts[3]
                state_val[7,j]=1
                state_val[8,j]=1
            state_vals.append(state_val)

        temp_masks, trajs, _ = getFragmentedTraj(surfaceBSpline,state_names,state_vals,np.array([0,0,0,1]),np.zeros((3,)),3)
        if self._horizontal:
            R_extra_tool = ScipyR.from_euler('x',-10,degrees=True)
        else:
            R_extra_tool = ScipyR.from_euler('y',-10,degrees=True)
        q_extra = R_extra_tool.as_quat()
        corrections = [0.0, 0.0, -5.0, q_extra[0], q_extra[1], q_extra[2], q_extra[3], -0.5, 0.0]
        fragmentedBehavior = constructConnectedTraj(surfaceBSpline,state_names,state_vals,temp_masks,corrections,self._time_sampling)
        return temp_masks,trajs,fragmentedBehavior

    def get_return(self,surfaceBSpline,un,vn):
        ang_relative_to_surface = 0 # degrees
        R_tool_surf = ScipyR.from_euler('z',180,degrees=True)
        q_ts = R_tool_surf.as_quat()

        segment = HybridSegment()
        segment.hybrid = False
        segment.num_samples = 10 # 2 seconds
        segment.state_names = ['x','y','z','qx','qy','qz','qw','valve']
        segment.original_vals = []

        retract_pt, n_hat, r_u_norm, r_v_norm = surfaceBSpline.calculate_surface_point(un, vn)

        #dot product between robot-retract_pt
        # trans = self._tfBuffer.lookup_transform(REFERENCE_FRAME,CAMERA_FRAME, rospy.Time()).transform.translation  
        # view_v = np.array(retract_pt - [trans.x, trans.y, trans.z])
        # if np.dot(view_v,n_hat) < 0:
        #     n_hat = -n_hat

        trans = self._tfBuffer.lookup_transform(REFERENCE_FRAME, "panda_ee", rospy.Time()).transform

        R_surf = ScipyR.from_matrix(np.hstack([r_u_norm.reshape((3,1)), r_v_norm.reshape((3,1)), n_hat.reshape((3,1))]))
        q_ret = (R_surf * R_tool_surf).as_quat()
        offset1 =  0.005 * n_hat
        starting = [retract_pt[0]+offset1[0], retract_pt[1]+offset1[1], retract_pt[2]+offset1[2], q_ret[0], q_ret[1], q_ret[2], q_ret[3],1]
        
        offset2 =  0.05 * n_hat
        # ending = [retract_pt[0]+offset2[0], retract_pt[1]+offset2[1], retract_pt[2]+offset2[2], q_ret[0], q_ret[1], q_ret[2], q_ret[3],0]
        ending = [trans.translation.x, trans.translation.y, trans.translation.z, trans.rotation.x, trans.rotation.y, trans.rotation.z, trans.rotation.w, 0]
        orig_vals = interpMultD(starting,ending,segment.num_samples)
        segment.corrections.append(np.zeros((len(segment.state_names),segment.num_samples)))
        segment.original_vals.append(orig_vals)
        return segment

    def get_uvs(self):
        n_passes = self._n_passes
        horizontal = self._horizontal
        margin = 0.05
        length = 1-2*margin
        us=[margin]
        vs=[margin]
        for i in range(n_passes):
            x = margin if i%2 else (1-margin)
            y = margin+length*i/(n_passes-1)
            val = [x,y]
            if horizontal:
                val = [y,x]
            us.append(val[0])
            vs.append(val[1])
            if i != (n_passes-1):
                x = margin if i%2 else (1-margin)
                y = margin+length*(i+1)/(n_passes-1)
                val = [x,y]
                if horizontal:
                    val = [y,x]
                us.append(val[0])
                vs.append(val[1])
                
        return us,vs

    def get_path(self):
        surfaceBSpline = self._surfaceBSpline
        print("getting uvs")
        us,vs = self.get_uvs()
        
        print("getting segments")
        self._mask,self._trajs, self._learnedSegments = self.get_segments(surfaceBSpline,us,vs)
        
        print("done")
        path = Path()
        path.header.frame_id = REFERENCE_FRAME
        for i,segment in enumerate(self._mask):
            for j,val in enumerate(segment):
                if val:
                    pose = PoseStamped()
                    pose.header.frame_id = REFERENCE_FRAME

                    pose.pose.position.x = self._trajs[i][0,j]
                    pose.pose.position.y = self._trajs[i][1,j]
                    pose.pose.position.z = self._trajs[i][2,j]
                    pose.pose.orientation.x = self._trajs[i][3,j]
                    pose.pose.orientation.y = self._trajs[i][4,j]
                    pose.pose.orientation.z = self._trajs[i][5,j]
                    pose.pose.orientation.w = self._trajs[i][6,j]
                    path.poses.append(pose)

        return path

    def get_pixel_from_pose(self, pose):
        if pose.header.frame_id != CAMERA_FRAME:
            pose = self._tfBuffer.transform(pose,CAMERA_FRAME)
            
        p = pose.pose.position
        if p.z < 0:
            return None
        p=[p.x,p.y,p.z]
        return self._camera_model.project3dToPixel(p)
    
    def run(self):
        rospy.spin()

    def signal_handler(self, signal, frame):
        sys.exit()

if __name__ == "__main__":
    rospy.init_node('spliner')
    spliner = Spliner()
    signal.signal(signal.SIGINT, spliner.signal_handler)
    spliner.run()